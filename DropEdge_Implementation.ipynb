{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing librarys**"
      ],
      "metadata": {
        "id": "Lm4GKY_EFjYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdBlr_BQOvwQ",
        "outputId": "36c47181-9744-4575-ff22-5a95bbdb2f1b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m0.9/1.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "EJusied-FsjB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kci6zsMFwb-Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "from torch_geometric.transforms import ToSparseTensor\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import MLP\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.nn.functional import softmax\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "from itertools import permutations\n",
        "from itertools import product\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GATv2Conv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download Datasets**"
      ],
      "metadata": {
        "id": "3HHQGow2Fxry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training, validation, and test sets\n",
        "train_ratio, val_ratio, test_ratio = 0.7, 0.1, 0.2"
      ],
      "metadata": {
        "id": "Rs-una_tFT4i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset --> citeseer\n",
        "dataset_cite = Planetoid(root='./cite', name='CiteSeer')\n",
        "\n",
        "cite_classes = dataset_cite.num_classes\n",
        "cite_features = dataset_cite.num_features\n",
        "\n",
        "# Calculate the number of all nodes\n",
        "nodes = dataset_cite[0].num_nodes\n",
        "\n",
        "# Shuffle the indices to create a random split\n",
        "indices = torch.randperm(nodes)\n",
        "\n",
        "# Calculate size of each split\n",
        "num_train = int(train_ratio * nodes)\n",
        "num_val = int(val_ratio * nodes)\n",
        "num_test = nodes - num_train - num_val\n",
        "\n",
        "# Create a mask for each split\n",
        "train_mask_cite = torch.zeros(nodes, dtype=torch.bool)\n",
        "val_mask_cite = torch.zeros(nodes, dtype=torch.bool)\n",
        "test_mask_cite = torch.zeros(nodes, dtype=torch.bool)\n",
        "\n",
        "# Convert the mask indices into true for split selection\n",
        "train_mask_cite[indices[: num_train]]=True\n",
        "val_mask_cite[indices[num_train : num_train+num_val]]=True\n",
        "test_mask_cite[indices[num_train+num_val :]]=True\n",
        "\n",
        "# Apply the calculated masks into the dataset\n",
        "dataset_cite[0].train_mask = train_mask_cite\n",
        "dataset_cite[0].val_mask = val_mask_cite\n",
        "dataset_cite[0].test_mask = test_mask_cite\n",
        "\n",
        "print(dataset_cite[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMrB_jnZFEtq",
        "outputId": "3aaf9fed-9446-40e0-9bc4-d780723a7185"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset --> corafull\n",
        "dataset_corafull = CitationFull(root='./cora', name='cora')\n",
        "\n",
        "cora_classes = dataset_corafull.num_classes\n",
        "cora_features = dataset_corafull.num_features\n",
        "\n",
        "# Calculate the number of all nodes\n",
        "nodes = dataset_corafull[0].num_nodes\n",
        "\n",
        "# Shuffle the indices to create a random split\n",
        "indices = torch.randperm(nodes)\n",
        "\n",
        "# Calculate size of each split\n",
        "num_train = int(train_ratio * nodes)\n",
        "num_val = int(val_ratio * nodes)\n",
        "num_test = nodes - num_train - num_val\n",
        "\n",
        "# Create a mask for each split\n",
        "train_mask_cora = torch.zeros(nodes, dtype=torch.bool)\n",
        "val_mask_cora = torch.zeros(nodes, dtype=torch.bool)\n",
        "test_mask_cora = torch.zeros(nodes, dtype=torch.bool)\n",
        "\n",
        "# Convert the mask indices into true for split selection\n",
        "train_mask_cora[indices[: num_train]]=True\n",
        "val_mask_cora[indices[num_train : num_train+num_val]]=True\n",
        "test_mask_cora[indices[num_train+num_val :]]=True\n",
        "\n",
        "dataset_cora = dataset_corafull[0]\n",
        "\n",
        "# Apply the calculated masks into the dataset\n",
        "dataset_cora.train_mask = train_mask_cora\n",
        "dataset_cora.val_mask = val_mask_cora\n",
        "dataset_cora.test_mask = test_mask_cora\n",
        "\n",
        "\n",
        "dataset_cora.num_classes = dataset_corafull.num_classes\n",
        "dataset_cora.cora_features = dataset_corafull.num_features\n",
        "\n",
        "print(dataset_cora)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_qapdh4FNSs",
        "outputId": "8707b14e-73cb-435c-c10f-ec9e179a02fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/abojchevski/graph2gauss/raw/master/data/cora.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[19793, 8710], edge_index=[2, 126842], y=[19793], train_mask=[19793], val_mask=[19793], test_mask=[19793], num_classes=70, cora_features=8710)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DropEdge Method**"
      ],
      "metadata": {
        "id": "fvRQpbxmF5k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropEdgeLayer(nn.Module):\n",
        "    def __init__(self, drop_rate):\n",
        "        super(DropEdgeLayer, self).__init__()\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        edge_mask = torch.bernoulli(torch.full((edge_index.size(1),), 1 - self.drop_rate)).bool()\n",
        "        return edge_index[:, edge_mask]\n",
        "\n",
        "class GCNTwoLayer_DropEdge(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes, drop_rate, num_layers=2):\n",
        "        super(GCNTwoLayer_DropEdge, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define GCN layers\n",
        "        self.conv_layers = nn.ModuleList([GCNConv(num_features, hidden_channels)])\n",
        "        self.drop_edge_layers = nn.ModuleList([DropEdgeLayer(drop_rate) for _ in range(num_layers - 1)])\n",
        "        self.conv_layers.extend([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])\n",
        "        self.final_layer = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Apply DropEdge for each layer\n",
        "        for i in range(self.num_layers):\n",
        "            if i > 0:\n",
        "                edge_index = self.drop_edge_layers[i-1](edge_index)\n",
        "\n",
        "            x = self.conv_layers[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.final_layer(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "F3IkNNI1Pson"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_cite\n",
        "print('Dataset: CiteSeer')\n",
        "\n",
        "# Define training parameters\n",
        "# Define training parameters\n",
        "layer_size = 16\n",
        "epochs = 55\n",
        "learning_rate = 0.008\n",
        "drop_rate = 0.35\n",
        "\n",
        "# Define model\n",
        "model = GCNTwoLayer_DropEdge(dataset.num_features, layer_size, dataset.num_classes, drop_rate, num_layers=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model with hidden size({layer_size}):\\n\")\n",
        "print(f\"...Training Start...\")\n",
        "\n",
        "# Training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(dataset)\n",
        "    loss = criterion(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    _, predicted = out.max(dim=1)\n",
        "    correct = predicted[dataset.train_mask] == dataset.y[dataset.train_mask]\n",
        "    train_acc = correct.sum().item() / dataset.train_mask.sum().item()\n",
        "\n",
        "    # Printing Every 20 epochs\n",
        "    if ((epoch+1) % 10) == 0 or epoch+1 == epochs:\n",
        "        print(f\"Epoch {epoch+1}: loss={loss.item()}, training accuracy={train_acc}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.val_mask] == dataset.y[dataset.val_mask]\n",
        "    acc = correct.sum().item() / dataset.val_mask.sum().item()\n",
        "\n",
        "print(f\"...Training End...\")\n",
        "print(f\"\\nValidation accuracy= {acc}\")\n",
        "\n",
        "# Test Evaluation for the Best Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.test_mask] == dataset.y[dataset.test_mask]\n",
        "    acc = correct.sum().item() / dataset.test_mask.sum().item()\n",
        "\n",
        "print(f\"Test accuracy= {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGYVszyHDH-l",
        "outputId": "df54ab59-54cb-4002-dac0-525878322f90"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: CiteSeer\n",
            "Model with hidden size(16):\n",
            "\n",
            "...Training Start...\n",
            "Epoch 10: loss=1.105533480644226, training accuracy=0.575\n",
            "Epoch 20: loss=0.6705427765846252, training accuracy=0.7916666666666666\n",
            "Epoch 30: loss=0.3271563649177551, training accuracy=0.8666666666666667\n",
            "Epoch 40: loss=0.3873106837272644, training accuracy=0.875\n",
            "Epoch 50: loss=0.24747994542121887, training accuracy=0.925\n",
            "Epoch 55: loss=0.32405534386634827, training accuracy=0.9083333333333333\n",
            "...Training End...\n",
            "\n",
            "Validation accuracy= 0.648\n",
            "Test accuracy= 0.657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_cora\n",
        "print('Dataset: CoraFull')\n",
        "\n",
        "# Define training parameters\n",
        "# Define training parameters\n",
        "layer_size = 16\n",
        "epochs = 230\n",
        "learning_rate = 0.008\n",
        "drop_rate = 0.35\n",
        "\n",
        "# Define model\n",
        "model = GCNTwoLayer_DropEdge(dataset.num_features, layer_size, dataset.num_classes, drop_rate, num_layers=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model with hidden size({layer_size}):\\n\")\n",
        "print(f\"...Training Start...\")\n",
        "\n",
        "# Training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(dataset)\n",
        "    loss = criterion(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    _, predicted = out.max(dim=1)\n",
        "    correct = predicted[dataset.train_mask] == dataset.y[dataset.train_mask]\n",
        "    train_acc = correct.sum().item() / dataset.train_mask.sum().item()\n",
        "\n",
        "    # Printing Every 20 epochs\n",
        "    if ((epoch+1) % 20) == 0 or epoch+1 == epochs:\n",
        "        print(f\"Epoch {epoch+1}: loss={loss.item()}, training accuracy={train_acc}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.val_mask] == dataset.y[dataset.val_mask]\n",
        "    acc = correct.sum().item() / dataset.val_mask.sum().item()\n",
        "\n",
        "print(f\"...Training End...\")\n",
        "print(f\"\\nValidation accuracy= {acc}\")\n",
        "\n",
        "# Test Evaluation for the Best Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.test_mask] == dataset.y[dataset.test_mask]\n",
        "    acc = correct.sum().item() / dataset.test_mask.sum().item()\n",
        "\n",
        "print(f\"Test accuracy= {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftEZgcl-DIA8",
        "outputId": "2162a3f7-48e3-4b50-cea0-9f935c0f4e07"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: CoraFull\n",
            "Model with hidden size(16):\n",
            "\n",
            "...Training Start...\n",
            "Epoch 20: loss=3.1894445419311523, training accuracy=0.20988812702995308\n",
            "Epoch 40: loss=2.3731744289398193, training accuracy=0.4042583904727535\n",
            "Epoch 60: loss=2.0079641342163086, training accuracy=0.48357993504150126\n",
            "Epoch 80: loss=1.8192232847213745, training accuracy=0.52948394081559\n",
            "Epoch 100: loss=1.701387643814087, training accuracy=0.5523637675929267\n",
            "Epoch 120: loss=1.6571295261383057, training accuracy=0.5592204980151569\n",
            "Epoch 140: loss=1.6148221492767334, training accuracy=0.5660772284373872\n",
            "Epoch 160: loss=1.5593241453170776, training accuracy=0.5818837964633706\n",
            "Epoch 180: loss=1.5439478158950806, training accuracy=0.5827499097798629\n",
            "Epoch 200: loss=1.5004788637161255, training accuracy=0.594947672320462\n",
            "Epoch 220: loss=1.4969053268432617, training accuracy=0.5891014074341393\n",
            "Epoch 230: loss=1.472802758216858, training accuracy=0.6023818116203536\n",
            "...Training End...\n",
            "\n",
            "Validation accuracy= 0.6538655886811521\n",
            "Test accuracy= 0.6519323061379136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GCN with 8 layers**"
      ],
      "metadata": {
        "id": "dmlKmZL8nYpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNWithDropEdge(nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes, drop_rate, num_layers=8):\n",
        "        super(GCNWithDropEdge, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.conv_layers = nn.ModuleList([GCNConv(num_features, hidden_channels)])\n",
        "        self.drop_edge_layers = nn.ModuleList([DropEdgeLayer(drop_rate) for _ in range(num_layers - 1)])\n",
        "        self.conv_layers.extend([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])\n",
        "        self.final_layer = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Apply GCN and DropEdge for each layer\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            if i < self.num_layers - 1:\n",
        "                edge_index = data.edge_index\n",
        "                edge_index = self.drop_edge_layers[i](edge_index)\n",
        "\n",
        "            x = self.conv_layers[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.final_layer(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "MWoUHahaDIDb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_cite\n",
        "print('Dataset: CiteSeer')\n",
        "\n",
        "# Define training parameters\n",
        "layer_size = 32\n",
        "epochs = 130\n",
        "learning_rate = 0.015\n",
        "drop_rate = 0.3\n",
        "\n",
        "# Define model\n",
        "model = GCNWithDropEdge(dataset.num_features, layer_size, dataset.num_classes, drop_rate, num_layers=8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model with hidden size({layer_size}):\\n\")\n",
        "print(f\"...Training Start...\")\n",
        "\n",
        "# Training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(dataset)\n",
        "    loss = criterion(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    _, predicted = out.max(dim=1)\n",
        "    correct = predicted[dataset.train_mask] == dataset.y[dataset.train_mask]\n",
        "    train_acc = correct.sum().item() / dataset.train_mask.sum().item()\n",
        "\n",
        "    # Printing Every 20 epochs\n",
        "    if ((epoch+1) % 10) == 0 or epoch+1 == epochs:\n",
        "        print(f\"Epoch {epoch+1}: loss={loss.item()}, training accuracy={train_acc}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.val_mask] == dataset.y[dataset.val_mask]\n",
        "    acc = correct.sum().item() / dataset.val_mask.sum().item()\n",
        "\n",
        "print(f\"...Training End...\")\n",
        "print(f\"\\nValidation accuracy= {acc}\")\n",
        "\n",
        "# Test Evaluation for the Best Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.test_mask] == dataset.y[dataset.test_mask]\n",
        "    acc = correct.sum().item() / dataset.test_mask.sum().item()\n",
        "\n",
        "print(f\"Test accuracy= {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-4OxBFiY_9l",
        "outputId": "cf692399-bdfe-4b40-b499-69eb1ebd3430"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: CiteSeer\n",
            "Model with hidden size(32):\n",
            "\n",
            "...Training Start...\n",
            "Epoch 10: loss=1.746225357055664, training accuracy=0.23333333333333334\n",
            "Epoch 20: loss=1.6936631202697754, training accuracy=0.38333333333333336\n",
            "Epoch 30: loss=1.1580437421798706, training accuracy=0.5333333333333333\n",
            "Epoch 40: loss=1.2659180164337158, training accuracy=0.5416666666666666\n",
            "Epoch 50: loss=0.9617868065834045, training accuracy=0.5833333333333334\n",
            "Epoch 60: loss=0.8618496656417847, training accuracy=0.6333333333333333\n",
            "Epoch 70: loss=0.8151241540908813, training accuracy=0.6416666666666667\n",
            "Epoch 80: loss=0.7987928986549377, training accuracy=0.6833333333333333\n",
            "Epoch 90: loss=0.7234742045402527, training accuracy=0.725\n",
            "Epoch 100: loss=0.7190862894058228, training accuracy=0.6833333333333333\n",
            "Epoch 110: loss=0.7791874408721924, training accuracy=0.75\n",
            "Epoch 120: loss=0.7186530828475952, training accuracy=0.75\n",
            "Epoch 130: loss=0.6421231031417847, training accuracy=0.7333333333333333\n",
            "...Training End...\n",
            "\n",
            "Validation accuracy= 0.508\n",
            "Test accuracy= 0.507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Skip Connection**"
      ],
      "metadata": {
        "id": "cao0o2e7oMEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNWithSkipConnection(nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels_list, num_classes):\n",
        "        super(GCNWithSkipConnection, self).__init__()\n",
        "        self.num_layers = len(hidden_channels_list) + 1\n",
        "\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels_list[0])\n",
        "        self.conv_layers = nn.ModuleList([GCNConv(hidden_channels_list[i - 1], hidden_channels_list[i]) for i in range(1, len(hidden_channels_list))])\n",
        "        self.final_layer = GCNConv(hidden_channels_list[-1], num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Apply GCN for each layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        # Add skip connection\n",
        "        x_old = x\n",
        "        for i, conv_layer in enumerate(self.conv_layers):\n",
        "            x = conv_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            # Skip connection\n",
        "            x = x + x_old\n",
        "            x_old = x\n",
        "        x = self.final_layer(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "wIZVEoLRZE0-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_cite\n",
        "print('Dataset: CiteSeer')\n",
        "\n",
        "# Define training parameters\n",
        "layer_size = [64,64,64,64,64,64,64]\n",
        "epochs = 130\n",
        "learning_rate = 0.015\n",
        "drop_rate = 0.3\n",
        "\n",
        "# Define model\n",
        "model = GCNWithSkipConnection(dataset.num_features, layer_size, dataset.num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model with hidden size({layer_size}):\\n\")\n",
        "print(f\"...Training Start...\")\n",
        "\n",
        "# Training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(dataset)\n",
        "    loss = criterion(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    _, predicted = out.max(dim=1)\n",
        "    correct = predicted[dataset.train_mask] == dataset.y[dataset.train_mask]\n",
        "    train_acc = correct.sum().item() / dataset.train_mask.sum().item()\n",
        "\n",
        "    # Printing Every 20 epochs\n",
        "    if ((epoch+1) % 10) == 0 or epoch+1 == epochs:\n",
        "        print(f\"Epoch {epoch+1}: loss={loss.item()}, training accuracy={train_acc}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.val_mask] == dataset.y[dataset.val_mask]\n",
        "    acc = correct.sum().item() / dataset.val_mask.sum().item()\n",
        "\n",
        "print(f\"...Training End...\")\n",
        "print(f\"\\nValidation accuracy= {acc}\")\n",
        "\n",
        "# Test Evaluation for the Best Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.test_mask] == dataset.y[dataset.test_mask]\n",
        "    acc = correct.sum().item() / dataset.test_mask.sum().item()\n",
        "\n",
        "print(f\"Test accuracy= {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7_4vKkhZE3d",
        "outputId": "f26364ce-56dc-4cb6-821e-16603429d491"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: CiteSeer\n",
            "Model with hidden size([64, 64, 64, 64, 64, 64, 64]):\n",
            "\n",
            "...Training Start...\n",
            "Epoch 10: loss=0.10233097523450851, training accuracy=0.9666666666666667\n",
            "Epoch 20: loss=0.007990460842847824, training accuracy=1.0\n",
            "Epoch 30: loss=0.0008284560171887279, training accuracy=1.0\n",
            "Epoch 40: loss=0.00019354723917786032, training accuracy=1.0\n",
            "Epoch 50: loss=5.326960308593698e-05, training accuracy=1.0\n",
            "Epoch 60: loss=3.8297210267046466e-05, training accuracy=1.0\n",
            "Epoch 70: loss=2.7101610612589866e-05, training accuracy=1.0\n",
            "Epoch 80: loss=1.98473317141179e-05, training accuracy=1.0\n",
            "Epoch 90: loss=1.7247510186280124e-05, training accuracy=1.0\n",
            "Epoch 100: loss=1.5208445802272763e-05, training accuracy=1.0\n",
            "Epoch 110: loss=1.3643709280586336e-05, training accuracy=1.0\n",
            "Epoch 120: loss=1.241555128217442e-05, training accuracy=1.0\n",
            "Epoch 130: loss=1.1339455340930726e-05, training accuracy=1.0\n",
            "...Training End...\n",
            "\n",
            "Validation accuracy= 0.59\n",
            "Test accuracy= 0.594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Skip Connection with DropEdge**"
      ],
      "metadata": {
        "id": "9ykbuKY1oU9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNWithResidualAndDropEdge(nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels_list, num_classes, drop_rate):\n",
        "        super(GCNWithResidualAndDropEdge, self).__init__()\n",
        "        self.num_layers = len(hidden_channels_list) + 1\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels_list[0])\n",
        "        self.drop_edge1 = DropEdgeLayer(drop_rate)\n",
        "        self.conv_layers = nn.ModuleList([GCNConv(hidden_channels_list[i - 1], hidden_channels_list[i]) for i in range(1, len(hidden_channels_list))])\n",
        "        self.drop_edge_layers = nn.ModuleList([DropEdgeLayer(drop_rate) for _ in range(len(hidden_channels_list))])\n",
        "        self.final_layer = GCNConv(hidden_channels_list[-1], num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Apply GCN and residual connection with DropEdge for each layer\n",
        "        x_old = x\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        # Add DropEdge\n",
        "        edge_index = self.drop_edge1(edge_index)\n",
        "        for i, (conv_layer, drop_edge_layer) in enumerate(zip(self.conv_layers, self.drop_edge_layers)):\n",
        "            edge_index = edge_index\n",
        "            x_residual = x\n",
        "            x = conv_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            # Add DropEdge\n",
        "            edge_index = drop_edge_layer(edge_index)\n",
        "            # Residual connection\n",
        "            x = x + x_residual\n",
        "        x = self.final_layer(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "TiBxL_5tZE50"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_cite\n",
        "print('Dataset: CiteSeer')\n",
        "\n",
        "# Define training parameters\n",
        "layer_size = [64,64,64,64,64,64,64]\n",
        "epochs = 130\n",
        "learning_rate = 0.015\n",
        "drop_rate = 0.3\n",
        "\n",
        "# Define model\n",
        "model = GCNWithSkipConnection(dataset.num_features, layer_size, dataset.num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Model with hidden size({layer_size}):\\n\")\n",
        "print(f\"...Training Start...\")\n",
        "\n",
        "# Training\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(dataset)\n",
        "    loss = criterion(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    _, predicted = out.max(dim=1)\n",
        "    correct = predicted[dataset.train_mask] == dataset.y[dataset.train_mask]\n",
        "    train_acc = correct.sum().item() / dataset.train_mask.sum().item()\n",
        "\n",
        "    # Printing Every 20 epochs\n",
        "    if ((epoch+1) % 10) == 0 or epoch+1 == epochs:\n",
        "        print(f\"Epoch {epoch+1}: loss={loss.item()}, training accuracy={train_acc}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.val_mask] == dataset.y[dataset.val_mask]\n",
        "    acc = correct.sum().item() / dataset.val_mask.sum().item()\n",
        "\n",
        "print(f\"...Training End...\")\n",
        "print(f\"\\nValidation accuracy= {acc}\")\n",
        "\n",
        "# Test Evaluation for the Best Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(dataset)\n",
        "    _, predicted = logits.max(dim=1)\n",
        "    correct = predicted[dataset.test_mask] == dataset.y[dataset.test_mask]\n",
        "    acc = correct.sum().item() / dataset.test_mask.sum().item()\n",
        "\n",
        "print(f\"Test accuracy= {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvpD2sPX9AN7",
        "outputId": "dcccf3f3-ef8d-4da2-8749-6456849b72f5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: CiteSeer\n",
            "Model with hidden size([64, 64, 64, 64, 64, 64, 64]):\n",
            "\n",
            "...Training Start...\n",
            "Epoch 10: loss=0.05564705654978752, training accuracy=0.975\n",
            "Epoch 20: loss=0.006867618765681982, training accuracy=1.0\n",
            "Epoch 30: loss=0.0009768388699740171, training accuracy=1.0\n",
            "Epoch 40: loss=0.00030855779186822474, training accuracy=1.0\n",
            "Epoch 50: loss=9.947941725840792e-05, training accuracy=1.0\n",
            "Epoch 60: loss=4.334590266807936e-05, training accuracy=1.0\n",
            "Epoch 70: loss=2.7358388251741417e-05, training accuracy=1.0\n",
            "Epoch 80: loss=2.383056744292844e-05, training accuracy=1.0\n",
            "Epoch 90: loss=2.0475650671869516e-05, training accuracy=1.0\n",
            "Epoch 100: loss=1.7695552742225118e-05, training accuracy=1.0\n",
            "Epoch 110: loss=1.5693574823671952e-05, training accuracy=1.0\n",
            "Epoch 120: loss=1.446012538508512e-05, training accuracy=1.0\n",
            "Epoch 130: loss=1.3347760614124127e-05, training accuracy=1.0\n",
            "...Training End...\n",
            "\n",
            "Validation accuracy= 0.61\n",
            "Test accuracy= 0.607\n"
          ]
        }
      ]
    }
  ]
}